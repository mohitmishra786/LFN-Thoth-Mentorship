{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2352153,"sourceType":"datasetVersion","datasetId":1420174}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Required Installation","metadata":{}},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm\nfrom spacy.training import Example\n!pip install pyspark\n!pip install spacy-lookups-data\n!python -m spacy download en_core_web_md","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:39:52.761853Z","iopub.execute_input":"2024-04-06T17:39:52.762429Z","iopub.status.idle":"2024-04-06T17:41:36.059643Z","shell.execute_reply.started":"2024-04-06T17:39:52.762383Z","shell.execute_reply":"2024-04-06T17:41:36.058202Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488512 sha256=74a034435d66fa7c3f50e438fa22a0a55d495d6848e4b5d20cf73f15a2eb5e7f\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\nCollecting spacy-lookups-data\n  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy-lookups-data) (59.8.0)\nInstalling collected packages: spacy-lookups-data\nSuccessfully installed spacy-lookups-data-1.0.5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nCollecting en-core-web-md==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.6.0) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.6)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.65.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.5.7)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\nInstalling collected packages: en-core-web-md\nSuccessfully installed en-core-web-md-3.6.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark Session and Data Setup","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType, StructField, StringType\nimport re\n\n# Initialize a Spark session\nspark = SparkSession.builder.appName(\"LogAnalysis\").getOrCreate()\n#spark = SparkSession.builder.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate()\n\n# Define the schema for log entries\nschema = StructType([\n    StructField(\"Timestamp\", StringType(), True),\n    StructField(\"Log Level\", StringType(), True),\n    StructField(\"Component\", StringType(), True),\n    StructField(\"Operation Type\", StringType(), True),\n    StructField(\"Block ID\", StringType(), True),\n    StructField(\"Source Address\", StringType(), True),\n    StructField(\"Destination Address\", StringType(), True)\n])\n\n# Regular expression pattern to extract information\npattern = r'(\\d{6} \\d{6}) (\\d+) (\\w+) (\\S+): (\\w+) block (\\S+) src: (\\S+) dest: (\\S+)'\n\n# List to store extracted log entries\nextracted_data = []\n\n# Read and process the log file\nwith open('/kaggle/input/loghub-hadoop-distributed-file-system-log-data/HDFS_1/HDFS.log', 'r') as log_file:\n    for line in log_file:\n        match = re.match(pattern, line)\n        if match:\n            timestamp, log_level, component, _, operation_type, block_id, source_addr, dest_addr = match.groups()\n            log_entry = (timestamp, log_level, component, operation_type, block_id, source_addr, dest_addr)\n            extracted_data.append(log_entry)\n\n# Create a DataFrame using the defined schema\ndf = spark.createDataFrame(extracted_data, schema=schema)\n\n# Convert the \"Timestamp\" column to date format\ndf = df.withColumn(\"Timestamp\", to_date(df[\"Timestamp\"], \"yyMMdd HHmmss\"))\n\nprint(\"df Schema:\")\ndf.printSchema()\n\n# Extract the block number from Block ID using a regular expression\nblock_pattern = r'blk_-(\\d+)'\ndf = df.withColumn(\"Block\", regexp_extract(\"Block ID\", block_pattern, 1))\n\n# Read the \"anomaly_label.csv\" file\nanomaly_label_df = spark.read.csv('/kaggle/input/loghub-hadoop-distributed-file-system-log-data/HDFS_1/anomaly_label.csv', header=True, inferSchema=True)\nprint(\"anomaly_label_df Schema:\")\nanomaly_label_df.printSchema()\n\n# Join the DataFrames on BlockId\nmerged_df = df.join(anomaly_label_df, df[\"Block ID\"] == anomaly_label_df[\"BlockId\"], \"left\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T17:41:36.065905Z","iopub.execute_input":"2024-04-06T17:41:36.067459Z","iopub.status.idle":"2024-04-06T17:42:36.973489Z","shell.execute_reply.started":"2024-04-06T17:41:36.067414Z","shell.execute_reply":"2024-04-06T17:42:36.972280Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/06 17:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"df Schema:\nroot\n |-- Timestamp: date (nullable = true)\n |-- Log Level: string (nullable = true)\n |-- Component: string (nullable = true)\n |-- Operation Type: string (nullable = true)\n |-- Block ID: string (nullable = true)\n |-- Source Address: string (nullable = true)\n |-- Destination Address: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 1:============================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"anomaly_label_df Schema:\nroot\n |-- BlockId: string (nullable = true)\n |-- Label: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"merged_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:42:36.974957Z","iopub.execute_input":"2024-04-06T17:42:36.975329Z","iopub.status.idle":"2024-04-06T17:42:47.398802Z","shell.execute_reply.started":"2024-04-06T17:42:36.975298Z","shell.execute_reply":"2024-04-06T17:42:47.397552Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"24/04/06 17:42:37 WARN TaskSetManager: Stage 2 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n24/04/06 17:42:43 WARN PythonRunner: Detected deadlock while completing task 3.0 in stage 2 (TID 8): Attempting to kill Python Worker\n24/04/06 17:42:43 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 5): Attempting to kill Python Worker\n24/04/06 17:42:43 WARN PythonRunner: Detected deadlock while completing task 2.0 in stage 2 (TID 7): Attempting to kill Python Worker\n24/04/06 17:42:43 WARN PythonRunner: Detected deadlock while completing task 1.0 in stage 2 (TID 6): Attempting to kill Python Worker\n                                                                                \r","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[Row(Timestamp=datetime.date(2008, 11, 9), Log Level='143', Component='INFO', Operation Type='Receiving', Block ID='blk_-1608999687919862906', Source Address='/10.250.19.102:54106', Destination Address='/10.250.19.102:50010', Block='1608999687919862906', BlockId='blk_-1608999687919862906', Label='Normal'),\n Row(Timestamp=datetime.date(2008, 11, 9), Log Level='143', Component='INFO', Operation Type='Receiving', Block ID='blk_-1608999687919862906', Source Address='/10.250.10.6:40524', Destination Address='/10.250.10.6:50010', Block='1608999687919862906', BlockId='blk_-1608999687919862906', Label='Normal'),\n Row(Timestamp=datetime.date(2008, 11, 9), Log Level='145', Component='INFO', Operation Type='Receiving', Block ID='blk_-1608999687919862906', Source Address='/10.250.14.224:42420', Destination Address='/10.250.14.224:50010', Block='1608999687919862906', BlockId='blk_-1608999687919862906', Label='Normal'),\n Row(Timestamp=datetime.date(2008, 11, 10), Log Level='10371', Component='INFO', Operation Type='Receiving', Block ID='blk_-548366025452983323', Source Address='/10.251.193.224:53276', Destination Address='/10.251.193.224:50010', Block='548366025452983323', BlockId='blk_-548366025452983323', Label='Normal'),\n Row(Timestamp=datetime.date(2008, 11, 11), Log Level='22544', Component='INFO', Operation Type='Receiving', Block ID='blk_-6401807258480830455', Source Address='/10.251.91.229:58346', Destination Address='/10.251.91.229:50010', Block='6401807258480830455', BlockId='blk_-6401807258480830455', Label='Normal')]"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:42:47.402014Z","iopub.execute_input":"2024-04-06T17:42:47.403356Z","iopub.status.idle":"2024-04-06T17:42:47.417306Z","shell.execute_reply.started":"2024-04-06T17:42:47.403302Z","shell.execute_reply":"2024-04-06T17:42:47.415927Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['Timestamp',\n 'Log Level',\n 'Component',\n 'Operation Type',\n 'Block ID',\n 'Source Address',\n 'Destination Address',\n 'Block',\n 'BlockId',\n 'Label']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning The Dataset","metadata":{}},{"cell_type":"code","source":"# Drop duplicate entries based on BlockId\ndeduplicated_df = merged_df.dropDuplicates([\"BlockId\"])\n\n# Select specific columns to create a new DataFrame\ncleaned_df = deduplicated_df.select(\"Label\", \"BlockId\", \"Timestamp\", \"Log Level\", \"Operation Type\")\n\n# Show the new DataFrame\ncleaned_df.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:42:47.419913Z","iopub.execute_input":"2024-04-06T17:42:47.421504Z","iopub.status.idle":"2024-04-06T17:43:05.036270Z","shell.execute_reply.started":"2024-04-06T17:42:47.421447Z","shell.execute_reply":"2024-04-06T17:43:05.035219Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"24/04/06 17:42:47 WARN TaskSetManager: Stage 7 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 11:==============>                                           (1 + 3) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------+--------------------+----------+---------+--------------+\n|  Label|             BlockId| Timestamp|Log Level|Operation Type|\n+-------+--------------------+----------+---------+--------------+\n| Normal|blk_-100001458415...|2008-11-10|     5887|     Receiving|\n| Normal|blk_-100004553717...|2008-11-11|    19873|     Receiving|\n| Normal|blk_-100021874837...|2008-11-10|    12321|     Receiving|\n| Normal|blk_-100028489120...|2008-11-10|    11134|     Receiving|\n| Normal|blk_-100029668880...|2008-11-10|    12376|     Receiving|\n| Normal|blk_-100032145436...|2008-11-10|    12401|     Receiving|\n| Normal|blk_-100041069666...|2008-11-11|    17873|     Receiving|\n| Normal|blk_-100058394360...|2008-11-11|    19134|     Receiving|\n| Normal|blk_-100080480388...|2008-11-11|    21852|     Receiving|\n| Normal|blk_-100088503271...|2008-11-11|    18893|     Receiving|\n| Normal|blk_-100092734476...|2008-11-09|     1385|     Receiving|\n| Normal|blk_-100097373580...|2008-11-10|    11195|     Receiving|\n| Normal|blk_-100101669119...|2008-11-10|     5428|     Receiving|\n| Normal|blk_-100108299003...|2008-11-11|    26899|     Receiving|\n| Normal|blk_-100113813561...|2008-11-10|     5417|     Receiving|\n| Normal|blk_-100121484143...|2008-11-10|    14414|     Receiving|\n| Normal|blk_-100123369602...|2008-11-10|    15817|     Receiving|\n| Normal|blk_-100127896875...|2008-11-11|    21614|     Receiving|\n|Anomaly|blk_-100129976491...|2008-11-09|     1590|     Receiving|\n| Normal|blk_-100138586392...|2008-11-11|    23926|     Receiving|\n+-------+--------------------+----------+---------+--------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"count = cleaned_df.select(\"BlockId\").distinct().count()\nprint(\"Number of unique BlockIds:\", count)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:05.037519Z","iopub.execute_input":"2024-04-06T17:43:05.037903Z","iopub.status.idle":"2024-04-06T17:43:16.034686Z","shell.execute_reply.started":"2024-04-06T17:43:05.037867Z","shell.execute_reply":"2024-04-06T17:43:16.033430Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"24/04/06 17:43:05 WARN TaskSetManager: Stage 16 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 24:>                                                         (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"Number of unique BlockIds: 575061\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"cleaned_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:16.035977Z","iopub.execute_input":"2024-04-06T17:43:16.036401Z","iopub.status.idle":"2024-04-06T17:43:16.052646Z","shell.execute_reply.started":"2024-04-06T17:43:16.036360Z","shell.execute_reply":"2024-04-06T17:43:16.051018Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['Label', 'BlockId', 'Timestamp', 'Log Level', 'Operation Type']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Taking Only Sample For Training\n\n- As we have total 575061 rows but it will be very hard and time consuming for now to train it. So for the purpose of coding challenge i will reduce the dataset","metadata":{}},{"cell_type":"code","source":"sampled_df = cleaned_df.sample(fraction=0.01)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:16.072407Z","iopub.execute_input":"2024-04-06T17:43:16.073112Z","iopub.status.idle":"2024-04-06T17:43:16.093353Z","shell.execute_reply.started":"2024-04-06T17:43:16.073064Z","shell.execute_reply":"2024-04-06T17:43:16.091849Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sampled_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:16.098351Z","iopub.execute_input":"2024-04-06T17:43:16.099276Z","iopub.status.idle":"2024-04-06T17:43:25.616758Z","shell.execute_reply.started":"2024-04-06T17:43:16.099227Z","shell.execute_reply":"2024-04-06T17:43:25.615300Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"24/04/06 17:43:16 WARN TaskSetManager: Stage 30 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"5861"},"metadata":{}}]},{"cell_type":"code","source":"sampled_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:25.618973Z","iopub.execute_input":"2024-04-06T17:43:25.619462Z","iopub.status.idle":"2024-04-06T17:43:25.633405Z","shell.execute_reply.started":"2024-04-06T17:43:25.619405Z","shell.execute_reply":"2024-04-06T17:43:25.631246Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Label', 'BlockId', 'Timestamp', 'Log Level', 'Operation Type']"},"metadata":{}}]},{"cell_type":"code","source":"sampled_df.describe().show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:25.635476Z","iopub.execute_input":"2024-04-06T17:43:25.636370Z","iopub.status.idle":"2024-04-06T17:43:39.358741Z","shell.execute_reply.started":"2024-04-06T17:43:25.636319Z","shell.execute_reply":"2024-04-06T17:43:39.355222Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"24/04/06 17:43:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n24/04/06 17:43:26 WARN TaskSetManager: Stage 44 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 52:=============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------+-------+--------------------+------------------+--------------+\n|summary|  Label|             BlockId|         Log Level|Operation Type|\n+-------+-------+--------------------+------------------+--------------+\n|  count|   5865|                5865|              5865|          5865|\n|   mean|   NULL|                NULL|15701.821994884911|          NULL|\n| stddev|   NULL|                NULL|  7656.92725275512|          NULL|\n|    min|Anomaly|blk_-100127896875...|              1000|     Receiving|\n|    max| Normal|blk_9965052949101...|              9952|     Receiving|\n+-------+-------+--------------------+------------------+--------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom spacy.tokens import DocBin\nfrom sklearn.model_selection import train_test_split\n\ndata = sampled_df.toPandas()\n\n# Define the entities and patterns to anonymize\nentities = [\"BlockId\"]\npatterns = [{\"label\": \"KEYWORD\", \"pattern\": [{\"LOWER\": \"receiving\"}]}]\n\n# Prepare data for spaCy\ndef prepare_data(data):\n    train_data = []\n    for index, row in data.iterrows():\n        text = f\"{row['Label']} {row['BlockId']} {row['Timestamp']} {row['Log Level']} {row['Operation Type']}\"\n        ents = [(row['BlockId'].start(), row['BlockId'].end(), \"BlockId\")]\n        train_data.append((text, {\"entities\": ents}))\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:39.360652Z","iopub.execute_input":"2024-04-06T17:43:39.361178Z","iopub.status.idle":"2024-04-06T17:43:54.620289Z","shell.execute_reply.started":"2024-04-06T17:43:39.361134Z","shell.execute_reply":"2024-04-06T17:43:54.619133Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"24/04/06 17:43:40 WARN TaskSetManager: Stage 58 contains a task of very large size (49471 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"import re\ndef prepare_data_(data):\n    train_data = []\n    block_id_pattern = re.compile(\"blk_[-\\d]+\")  # Adjust the pattern as needed\n    for index, row in data.iterrows():\n        text = f\"{row['Label']} {row['BlockId']} {row['Timestamp']} {row['Log Level']} {row['Operation Type']}\"\n        match = block_id_pattern.search(text)\n        if match:\n            start, end = match.span()\n            ents = [(start, end, \"BlockId\")]\n            train_data.append((text, {\"entities\": ents}))\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:54.621872Z","iopub.execute_input":"2024-04-06T17:43:54.623877Z","iopub.status.idle":"2024-04-06T17:43:54.633123Z","shell.execute_reply.started":"2024-04-06T17:43:54.623809Z","shell.execute_reply":"2024-04-06T17:43:54.631301Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Data","metadata":{}},{"cell_type":"code","source":"# Split data into train and test sets\ntrain_df, test_df = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:54.636397Z","iopub.execute_input":"2024-04-06T17:43:54.637481Z","iopub.status.idle":"2024-04-06T17:43:54.659606Z","shell.execute_reply.started":"2024-04-06T17:43:54.637418Z","shell.execute_reply":"2024-04-06T17:43:54.658512Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Prepare training and testing data\ntrain_data = prepare_data_(train_df)\ntest_data = prepare_data_(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:54.661346Z","iopub.execute_input":"2024-04-06T17:43:54.662467Z","iopub.status.idle":"2024-04-06T17:43:55.089075Z","shell.execute_reply.started":"2024-04-06T17:43:54.662429Z","shell.execute_reply":"2024-04-06T17:43:55.087952Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"# Load the spaCy model\nnlp = spacy.load(\"en_core_web_md\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:55.090551Z","iopub.execute_input":"2024-04-06T17:43:55.091035Z","iopub.status.idle":"2024-04-06T17:43:57.242788Z","shell.execute_reply.started":"2024-04-06T17:43:55.090992Z","shell.execute_reply":"2024-04-06T17:43:57.241557Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Add the entity ruler to the pipeline\nruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\nruler.add_patterns(patterns)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:57.244345Z","iopub.execute_input":"2024-04-06T17:43:57.244705Z","iopub.status.idle":"2024-04-06T17:43:57.269016Z","shell.execute_reply.started":"2024-04-06T17:43:57.244674Z","shell.execute_reply":"2024-04-06T17:43:57.267668Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Convert the training data to DocBin format\ndb = DocBin()\nfor text, annotations in train_data:\n    doc = nlp.make_doc(text)\n    ents = []\n    for start, end, label in annotations[\"entities\"]:\n        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n        if span is not None:\n            ents.append(span)\n    doc.ents = ents\n    db.add(doc)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:57.270427Z","iopub.execute_input":"2024-04-06T17:43:57.270802Z","iopub.status.idle":"2024-04-06T17:43:58.559528Z","shell.execute_reply.started":"2024-04-06T17:43:57.270772Z","shell.execute_reply":"2024-04-06T17:43:58.558289Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Save the DocBin object\ndb.to_disk(\"./train.spacy\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:43:58.561243Z","iopub.execute_input":"2024-04-06T17:43:58.561578Z","iopub.status.idle":"2024-04-06T17:43:58.673415Z","shell.execute_reply.started":"2024-04-06T17:43:58.561551Z","shell.execute_reply":"2024-04-06T17:43:58.672301Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Train the NER model\nner = nlp.get_pipe(\"ner\")\noptimizer = nlp.create_optimizer()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:49:29.211110Z","iopub.execute_input":"2024-04-06T17:49:29.211549Z","iopub.status.idle":"2024-04-06T17:49:29.224500Z","shell.execute_reply.started":"2024-04-06T17:49:29.211518Z","shell.execute_reply":"2024-04-06T17:49:29.222946Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Disable other pipeline components during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):\n    # Calculate total number of training steps\n    total_steps = len(train_data)\n    \n    # Use tqdm to create a progress bar\n    with tqdm(total=total_steps, desc=\"Training NER\") as pbar:\n        for itn in range(5):\n            print(\"count\", itn)\n            random.shuffle(train_data)\n            losses = {}\n            for text, annotations in train_data:\n                doc = nlp.make_doc(text)\n                example = Example.from_dict(doc, annotations)\n                ner.update([example], drop=0.5, losses=losses)\n                pbar.update(1)  # Update progress bar after each step\n            print(losses)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:03:21.210005Z","iopub.execute_input":"2024-04-06T18:03:21.210497Z","iopub.status.idle":"2024-04-06T18:08:26.789125Z","shell.execute_reply.started":"2024-04-06T18:03:21.210462Z","shell.execute_reply":"2024-04-06T18:08:26.787887Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"Training NER:   0%|          | 7/4686 [00:00<01:09, 67.63it/s]","output_type":"stream"},{"name":"stdout","text":"count 0\n","output_type":"stream"},{"name":"stderr","text":"Training NER: 4698it [01:01, 75.92it/s]                          ","output_type":"stream"},{"name":"stdout","text":"{'ner': 7.658487340958925e-05}\ncount 1\n","output_type":"stream"},{"name":"stderr","text":"Training NER: 9387it [02:02, 75.80it/s]","output_type":"stream"},{"name":"stdout","text":"{'ner': 1.7471418492289965}\ncount 2\n","output_type":"stream"},{"name":"stderr","text":"Training NER: 14069it [03:02, 76.98it/s]","output_type":"stream"},{"name":"stdout","text":"{'ner': 0.009784093102804883}\ncount 3\n","output_type":"stream"},{"name":"stderr","text":"Training NER: 18752it [04:04, 75.28it/s]","output_type":"stream"},{"name":"stdout","text":"{'ner': 0.32134077670114386}\ncount 4\n","output_type":"stream"},{"name":"stderr","text":"Training NER: 23430it [05:05, 76.68it/s]","output_type":"stream"},{"name":"stdout","text":"{'ner': 0.7968169886826436}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Saving Model","metadata":{}},{"cell_type":"code","source":"# Save the trained model\nnlp.to_disk(\"./main_ner_model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:08:29.720664Z","iopub.execute_input":"2024-04-06T18:08:29.721140Z","iopub.status.idle":"2024-04-06T18:08:30.427689Z","shell.execute_reply.started":"2024-04-06T18:08:29.721105Z","shell.execute_reply":"2024-04-06T18:08:30.426242Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Test the model\ntest_texts = [f\"{row['Label']} {row['BlockId']} {row['Timestamp']} {row['Log Level']} {row['Operation Type']}\" for index, row in test_df.iterrows()]\ndocs = list(nlp.pipe(test_texts))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:08:35.544239Z","iopub.execute_input":"2024-04-06T18:08:35.544634Z","iopub.status.idle":"2024-04-06T18:08:37.216795Z","shell.execute_reply.started":"2024-04-06T18:08:35.544603Z","shell.execute_reply":"2024-04-06T18:08:37.215646Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"len(docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:08:45.554336Z","iopub.execute_input":"2024-04-06T18:08:45.554749Z","iopub.status.idle":"2024-04-06T18:08:45.563295Z","shell.execute_reply.started":"2024-04-06T18:08:45.554718Z","shell.execute_reply":"2024-04-06T18:08:45.561869Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"1172"},"metadata":{}}]},{"cell_type":"code","source":"# # Print entities for manual inspection\n# for doc in docs:\n#     for ent in doc.ents:\n#         print(ent.text, ent.label_)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T17:56:28.384748Z","iopub.execute_input":"2024-04-06T17:56:28.385240Z","iopub.status.idle":"2024-04-06T17:56:28.391090Z","shell.execute_reply.started":"2024-04-06T17:56:28.385208Z","shell.execute_reply":"2024-04-06T17:56:28.389821Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing Model","metadata":{}},{"cell_type":"code","source":"def anonymize_block_ids(text):\n    doc = nlp(text)\n    anonymized_text = []\n    last_idx = 0\n    for ent in doc.ents:\n        if ent.label_ == \"BlockId\":\n            # Anonymize BlockId with a generic string\n            anonymized_text.append(text[last_idx:ent.start_char])\n            anonymized_text.append(\"[BLOCK_ID]\")\n            last_idx = ent.end_char\n    anonymized_text.append(text[last_idx:])\n    return \"\".join(anonymized_text)\n\n# Test Example\ntest_text = \"Normal blk_-100001458415... 2008-11-10 5887 Receiving\"\nanonymized_text = anonymize_block_ids(test_text)\nprint(\"Original Text:\", test_text)\nprint(\"Anonymized Text:\", anonymized_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:08:49.271534Z","iopub.execute_input":"2024-04-06T18:08:49.271962Z","iopub.status.idle":"2024-04-06T18:08:49.293292Z","shell.execute_reply.started":"2024-04-06T18:08:49.271929Z","shell.execute_reply":"2024-04-06T18:08:49.291487Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Original Text: Normal blk_-100001458415... 2008-11-10 5887 Receiving\nAnonymized Text: Normal [BLOCK_ID][BLOCK_ID] 2008-11-10 5887 Receiving\n","output_type":"stream"}]},{"cell_type":"code","source":"# This will be our lookup table (in a real project, this would need secure storage and management)\nlookup_table = {\n    \"[BLOCK_ID_1]\": \"blk_-100001458415...\",\n    \"[BLOCK_ID_2]\": \"blk_-100021874837...\",\n    # ... more entries ...\n}\n\ndef retrieve_block_ids(text):\n    doc = nlp(text)\n    retrieved_text = []\n    last_idx = 0\n    for ent in doc.ents:\n        if ent.label_ == \"BlockId\":\n            anonymized_id = text[ent.start_char:ent.end_char]\n            original_id = lookup_table.get(anonymized_id)  # It will retrieve from lookup table\n            if original_id:\n                retrieved_text.append(text[last_idx:ent.start_char])\n                retrieved_text.append(original_id)\n                last_idx = ent.end_char\n            else:\n                # Handle cases where the anonymized ID is not found\n                retrieved_text.append(text[last_idx:ent.end_char])\n                last_idx = ent.end_char\n    retrieved_text.append(text[last_idx:])\n    return \"\".join(retrieved_text)\n\n# Test Example\nanonymized_text = \"Normal [BLOCK_ID_1] 2008-11-10 5887 Receiving\"\nretrieved_text = retrieve_block_ids(anonymized_text)\nprint(\"Anonymized Text:\", anonymized_text)\nprint(\"Retrieved Text:\", retrieved_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T18:11:08.882668Z","iopub.execute_input":"2024-04-06T18:11:08.884224Z","iopub.status.idle":"2024-04-06T18:11:08.904440Z","shell.execute_reply.started":"2024-04-06T18:11:08.884174Z","shell.execute_reply":"2024-04-06T18:11:08.902933Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Anonymized Text: Normal [BLOCK_ID_1] 2008-11-10 5887 Receiving\nRetrieved Text: Normal blk_-100001458415... 2008-11-10 5887 Receiving\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}